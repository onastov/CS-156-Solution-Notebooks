{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning From Data - Homework 5\n",
    "## Ognen Nastov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='hw5_images/hw5p1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Substituting: \n",
    "\n",
    "$$0.008 < 0.1^2\\left(1-\\frac{9}{N}\\right)$$\n",
    "\n",
    "$$N > \\frac{9}{1-\\frac{0.008}{0.1^2}} = 45$$\n",
    "\n",
    "Answer is 100, [c].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra: derive the equation (book exercise 3.4).\n",
    "\n",
    "a) Show that in sample estimate of $y$ is $X w^{*} + H \\epsilon$.\n",
    "\n",
    "In sample estimate $\\hat{y}$ is:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\hat{y} & = Hy = H(w^{*T}x + \\epsilon) \\\\\n",
    "& = H w^{*T}x + H\\epsilon =  Xw^{*} + H \\epsilon\n",
    "\\end{aligned}$$\n",
    "\n",
    "since $H w^{*T}x = H X w^{*} = X (X^T X)^{-1} (X^T X) w^{*} = X w^{*}$\n",
    "\n",
    "b) Show that in-sample error $\\hat{y}-y$ can be expressed as matrix times $\\epsilon$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\hat{y}-y = Hy-y & = X w^* + H E - (X w^* + E) \\\\\n",
    "& = (H-I)E\n",
    "\\end{aligned}$$\n",
    "\n",
    "c) Express $E_{in}(w_{lin})$ in terms of $\\epsilon$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "E_{in}(w_{lin}) & = \\frac{1}{N} {||\\hat{y} - y||}^2 \\\\\n",
    "& = \\frac{1}{N}(\\hat{y}-y)^T (\\hat{y}-y) \\\\\n",
    "& = \\frac{1}{N} E^T (H-I)^T (H-I) E \\\\\n",
    "& = \\frac{1}{N} E^T (I-H) E \\\\\n",
    "& = \\frac{1}{N} E^T E - \\frac{1}{N} E^T H E\n",
    "\\end{aligned}$$\n",
    "\n",
    "since $H$ is symmetric, i.e. $H^T=H$, and also since $(I-H)^K = (I-H)$\n",
    "\n",
    "d) Prove that $\\mathbb E_D[E_{in}(w_{lin})] = \\sigma^2 \\left(1-\\frac{d+1}{N}\\right)$ using (c) and the independence of $\\epsilon_1,\\cdots,\\epsilon_N$. Hint: use $\\text{trace}(H) = d+1$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb E_D[E_{in}(w_{lin})] & = \\mathbb E_D\\left[\\frac{1}{N}E^T E\\right] - \\mathbb E_D\\left[\\frac{1}{N}E^T H E\\right] \\\\\n",
    "& = \\frac{1}{N} N \\sigma^2 - \\mathbb E_D\\left[\\frac{1}{N} E^T H E\\right] \\\\\n",
    "& = \\frac{1}{N} N \\sigma^2 - \\frac{1}{N} \\mathbb E_D\\left[\\sum_{i=1}^N H_{ii} \\epsilon_i^2 + \\sum_{i,j}^{i≠j} H_{ij} \\epsilon_i \\epsilon_j \\right] \\\\\n",
    "& = \\frac{1}{N} N \\sigma^2 - \\frac{1}{N}\\text{trace}(H) \\sigma^2 \\\\ \n",
    "& = \\sigma^2 \\left(1 - \\frac{d+1}{N}\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "since $\\mathbb E_D[\\sum_{i,j}^{i≠j} H_{ij}\\epsilon_i \\epsilon_j] = 0$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='hw5_images/hw5p2a.png'>\n",
    "<img src='hw5_images/hw5p2b.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "$$g(x) = \\text{sign}(\\tilde{w}^T \\Phi(x)) = \\text{sign}(\\tilde{w}_0 + \\tilde{w}_1 x_1^2 + \\tilde{w}_2 x_2^2)$$\n",
    "\n",
    "For $x_1=0$, $g(x)$ is always $+1$. Which means $\\tilde{w}_2 > 0$.  \n",
    "For $x_2=0$, $g(x)$ is $+1$ at $(0,0)$, and negative for large positive or negative $x_1$. Which means $\\tilde{w}_1 < 0$.\n",
    "\n",
    "Answer is [d].\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='hw5_images/hw5p3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "$d+1 = 3$, thus $d_{vc} = d+1 = 3$\n",
    "\n",
    "$\\tilde{d}+1 = 15$, thus $\\tilde{d}_{vc} ≤ \\tilde{d}+1 = 15$\n",
    "\n",
    "Answer is 15, i.e. [c].\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='hw5_images/hw5p4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial u} = 2(u e^v - 2 v e^{-u}) (e^v + 2 v e^{-u})$$\n",
    "\n",
    "Answer is [e].\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='hw5_images/hw5p5a.png'>\n",
    "<img src='hw5_images/hw5p5b.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial v} = 2 (u e^v - 2 v e^{-u}) (u e^v - 2 e^{-u})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# problems 5 & 6\n",
    "def E(u,v):\n",
    "    E = (u*np.exp(v) - 2*v*np.exp(-u))**2\n",
    "    return E;\n",
    "\n",
    "def dEdu(u,v):\n",
    "    value = 2*(u*np.exp(v) - 2*v*np.exp(-u))*(np.exp(v) + 2*v*np.exp(-u))\n",
    "    return value\n",
    "\n",
    "def dEdv(u,v):\n",
    "    value = 2*(u*np.exp(v) - 2*v*np.exp(-u))*(u*np.exp(v) - 2*np.exp(-u))\n",
    "    return value\n",
    "\n",
    "# gradient descent of E(u,v)\n",
    "def grad_descent(E, dEdu, dEdv, uv_init, max_iters, learning_rate, max_error):\n",
    "    iter = 0;\n",
    "    u = uv_init[0]\n",
    "    v = uv_init[1]\n",
    "    while iter < max_iters:\n",
    "        iter+=1\n",
    "        delta_u = -dEdu(u,v)\n",
    "        delta_v = -dEdv(u,v)\n",
    "        u+= learning_rate*delta_u\n",
    "        v+= learning_rate*delta_v\n",
    "        if E(u,v) < max_error:\n",
    "            break\n",
    "    print(f\"# iters = {iter}, error = {E(u,v)}, (u,v) = {(u,v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# iters = 10, error = 1.2086833944220747e-15, (u,v) = (0.04473629039778207, 0.023958714099141746)\n"
     ]
    }
   ],
   "source": [
    "grad_descent(E, dEdu, dEdv, (1,1), 20, 0.1, 1e-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is 10, i.e. [d].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='hw5_images/hw5p6.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# iters = 10, error = 1.2086833944220747e-15, (u,v) = (0.04473629039778207, 0.023958714099141746)\n"
     ]
    }
   ],
   "source": [
    "grad_descent(E, dEdu, dEdv, (1,1), 20, 0.1, 1e-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is [e].\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='hw5_images/hw5p7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord_descent(E, dEdu, dEdv, uv_init, max_iters, learning_rate, max_error):\n",
    "    iter = 0;\n",
    "    u = uv_init[0]\n",
    "    v = uv_init[1]\n",
    "    while iter < max_iters:\n",
    "        iter+=1\n",
    "        # step 1\n",
    "        delta_u = -dEdu(u,v)\n",
    "        u+= learning_rate*delta_u\n",
    "        # step 2\n",
    "        delta_v = -dEdv(u,v)\n",
    "        v+= learning_rate*delta_v\n",
    "        if E(u,v) < max_error:\n",
    "            break\n",
    "    print(f\"# iters = {iter}, error = {E(u,v)}, (u,v) = {(u,v)}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# iters = 15, error = 0.13981379199615315, (u,v) = (6.29707589930517, -2.852306954077811)\n"
     ]
    }
   ],
   "source": [
    "coord_descent(E, dEdu, dEdv, (1,1), 15, 0.1, 1e-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is 1e-1, [a].\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='hw5_images/hw5p8a.png'>\n",
    "<img src='hw5_images/hw5p8b.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Using `N_out = 2000`, `max_epochs = 10000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 8 & 9 - logistic regression with stochastic gradient descent\n",
    "\n",
    "# dimension of input space d\n",
    "# number of data points = N\n",
    "# X is shape [N, d+1] i.e. [N, 3]\n",
    "def make_data_set(N, d):\n",
    "   X = np.zeros([N,d])\n",
    "   X[:,0] = np.ones([N,])\n",
    "   for i in range(1,d):\n",
    "       Xi = np.random.uniform(-1,1,N)\n",
    "       X[:,i] = Xi\n",
    "   return (X)\n",
    "    \n",
    "# the target function\n",
    "# d = 2\n",
    "# sb is shape [2]\n",
    "def make_line():\n",
    "    x_1,y_1 = np.random.uniform(-1,1,2)\n",
    "    x_2,y_2 = np.random.uniform(-1,1,2)\n",
    "    slope = (y_1 - y_2) / (x_1 - x_2)\n",
    "    b = y_2 - slope * x_2\n",
    "    return (slope, b)\n",
    "\n",
    "# y is shape [N]    \n",
    "def classify_X(line, X):\n",
    "    slope, b = line\n",
    "    y = np.sign(X[:,1]*slope + b - X[:,2])\n",
    "    return(y)\n",
    "  \n",
    "# print flags\n",
    "print_lg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, line, y, w_init, max_epochs, eta, max_error, N_out):\n",
    "    N = np.size(X,0)\n",
    "    d = np.size(X,1) - 1\n",
    "    indices = np.arange(0,N)\n",
    "    w = w_init\n",
    "    n_epochs = 0\n",
    "    while (1):\n",
    "        n_epochs += 1\n",
    "        w_old = w\n",
    "        # permute the sample order\n",
    "        indices_perm = np.random.permutation(indices)\n",
    "        for index in indices_perm:\n",
    "            grad_e_n = -y[index]*X[index] / \\\n",
    "                (1 + np.exp(y[index]*np.dot(w,X[index])))\n",
    "            w = w - eta * grad_e_n\n",
    "        delta_w_norm = np.sqrt(np.sum((w - w_old)**2))\n",
    "        if delta_w_norm < max_error:\n",
    "            break\n",
    "    # generate out of sample\n",
    "    X_out = make_data_set(N_out, d+1)\n",
    "    y_out = classify_X(line, X_out)\n",
    "    # estimate E_out\n",
    "    E_out = (1/N_out)*np.sum(np.log(1 + np.exp(-y_out*np.dot(X_out,w))))\n",
    "    if (print_lg):\n",
    "        print(f\"Epochs = {n_epochs} , E_out = {E_out}\")\n",
    "    return (n_epochs, E_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 100, d = 2\n",
    "def run_exp(N, d, num_runs):\n",
    "    time_start = time.time()\n",
    "    n_epochs_runs = np.empty(0)\n",
    "    E_out_runs = np.empty(0)\n",
    "    while (num_runs > 0):\n",
    "        num_runs-= 1\n",
    "        # generate target\n",
    "        X = make_data_set(N, d+1)\n",
    "        line = make_line()\n",
    "        y = classify_X(line, X)\n",
    "        w_init = np.zeros(3)\n",
    "        # logistic regression with sgd\n",
    "        (n_epochs, E_out) = \\\n",
    "            logistic_regression(X, line, y, w_init, 2000, 0.01, 0.01, 10000)\n",
    "        n_epochs_runs = np.append(n_epochs_runs, n_epochs)\n",
    "        E_out_runs = np.append(E_out_runs, E_out)\n",
    "    n_epochs_mean = np.mean(n_epochs_runs)\n",
    "    E_out_mean = np.mean(E_out_runs)\n",
    "    print(f\"Mean epochs = {n_epochs_mean} , mean E_out = {E_out_mean}\")\n",
    "    time_end = time.time()\n",
    "    print(f\"Run time = {time_end - time_start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean epochs = 344.61 , mean E_out = 0.10423824951680664\n",
      "Run time = 85.0728600025177 seconds\n"
     ]
    }
   ],
   "source": [
    "run_exp(100, 2, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is 0.100 i.e. [d].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='hw5_images/hw5p9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "    \n",
    "Answer is 344.61 i.e. [a].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='hw5_images/hw5p10.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "For PLA, $h(x) = \\text{sign}(w^T x)$\n",
    "\n",
    "PLA update: $\\Delta_w = y_n x_n$\n",
    "\n",
    "We do an update only if a point is misclassified. Meaning the signs of $y_n$ and $w^Tx$ differ, i.e. $y_n w^Tx_n < 0$.\n",
    "\n",
    "No update (no change of weights) if the point is not misclassified. This means the error is 0.\n",
    "\n",
    "Thus a good error function is $\\max(0, -y_n w^Tx_n) = -\\min(0, -y_n w^Tx_n)$\n",
    " \n",
    "$e_n(w) = -\\min(0, y_n w^Tx_n)$\n",
    "\n",
    "Answer is [e].\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
