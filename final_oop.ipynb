{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning From Data - Final Exam (OOP version)\n",
    "## Ognen Nastov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "- For $Q=1$, $z = (1, x_1, x_2)$\n",
    "\n",
    "- For $Q=2$, $z = (1, x_1, x_2, x_1 x_2, x_1^2, x_2^2)$\n",
    "\n",
    "Etc.\n",
    "\n",
    "Not counting the $x_0=1$, when $Q$ increases by $1$, we have additional terms $x_1^i x_2^j$ with $i+j=Q$. The number of such terms is equal to the number of solutions to the equation.\n",
    "\n",
    "Analogous problem: how many ways can $Q$ pieces of candy be divided among $d$ children. Or how many $Q$ indistinguishable stars can be put into $d$ distinguishable bins.\n",
    "\n",
    "Represent the $Q$ candies as `*`, and represent the dividers specifying the bins as `|`. There are $d$ bins, thus there are $d-1$ dividers.\n",
    "\n",
    "E.g. for $Q=2$ and $d=2$, the set of configurations is: \\{`**|` , `*|*` , `|**`\\} which has 3 elements.\n",
    "\n",
    "The number of total objects (stars and dividers) is $Q+d-1$. Choosing the positions of the dividers leaves exactly $Q$ positions left for the stars. Thus, the number of different configurations is given by the $Q$ combinations of $Q+d-1$ configurations, i.e.\n",
    "\n",
    "$$\\text{# of ways} = \\binom{Q+d-1}{d-1} = \\frac{(Q+d-1)!}{Q! (d-1)!}$$\n",
    "\n",
    "If $d=2$, # ways = $\\binom{Q+d-1}{d-1} = Q+1$\n",
    "\n",
    "The total number of terms for $Q=10$ is therefore the sum $2+3+...+11$, i.e.\n",
    "\n",
    "$$\\text{dimensionality of Z} = \\sum_{i=2}^{11} i  = \\sum_{i=1}^{11} i - 1 = \\frac{11*12}{2} - 1 = 65$$\n",
    "\n",
    "The answer is none of the above, i.e. [e].\n",
    "\n",
    "---\n",
    "\n",
    "Problem 1 alternate answer:\n",
    "\n",
    "Dimensionality of poly transform of order $Q$ is given by $\\tilde{d} = \\frac{Q(Q+3)}{2} = \\frac{10*13}{2} = 65$\n",
    "\n",
    "(See book pp. 104 for reference.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "[a] A singleton $H$ with one hypothesis $h$, has $\\bar{g} = h$, thus $\\bar{g} \\in H$.\n",
    "\n",
    "[b] For $H$ being the set of all constant real valued hypotheses, the average of any number of $h \\in H$ will result also in a constant real valued hypothesis from the set, thus $\\bar{g} \\in H$.\n",
    "\n",
    "[c] For $H$ being the linear regression model, $h(x) = w^T x$. The average of these linear functions is also a linear function:\n",
    "\n",
    "$$\\bar{g}(x) = \\frac{1}{2}(h_1(x) + h_2(x)) = \\frac{1}{2}(w_1^T x + w_2^T x) = \\frac{1}{2}(w_1^T + w_2^T)x$$\n",
    "\n",
    "[d] For $H$ being the logistic regression model, $h(x) = \\theta(w^T x)$ with $\\theta(s) = \\frac{e^s}{1+e^s}$:\n",
    "\n",
    "$$h(x) = \\frac{e^{w^T x}}{1+e^{w^T x}}$$\n",
    "\n",
    "For the average $\\bar{g}(x) = \\frac{1}{2}(h_1(x) + h_2(x))$ to be representable as a logistic regression model, we need:\n",
    "\n",
    "$$\\bar{g}(x) = \\frac{1}{2}(h_1(x) + h_2(x)) = \\frac{1}{2} \\left( \\frac{e^{w_1^T x}}{1+e^{w_1^T x}} + \\frac{e^{w_2^T x}}{1+e^{w_2^T x}} \\right)$$\n",
    "\n",
    "to be equal to:\n",
    "\n",
    "$$\\frac{e^{w^T x}}{1+e^{w^T x}}$$\n",
    "\n",
    "which will not always be the case, and $\\bar{g} \\notin H$.\n",
    "\n",
    "Thus, answer is [d].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "[a] True.\n",
    "\n",
    "[b] True.\n",
    "\n",
    "[c] True.\n",
    "\n",
    "[d] False.\n",
    "\n",
    "[e] True.\n",
    "\n",
    "The answer is [d].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "[a] False.\n",
    "\n",
    "[b] False.\n",
    "\n",
    "[c] False.\n",
    "\n",
    "[d] True.\n",
    "\n",
    "[e] False.\n",
    "\n",
    "The answer is [d].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "$w_{lin}$ is the solution to the unconstrained minimization problem:\n",
    "\n",
    "$$\\min \\frac{1}{N} \\sum_{n=1}^N (w^T x_n - y_n)^2$$\n",
    "\n",
    "$w_{reg}$ is the solution to the constrained minimization problem:\n",
    "\n",
    "$$\\min \\frac{1}{N} \\sum_{n=1}^N (w^T x_n - y_n)^2$$ \n",
    "\n",
    "subject to $w^T \\Gamma^T \\Gamma w \\leq C$\n",
    "\n",
    "If $w_{lin}^T \\Gamma^T \\Gamma w_{lin} \\leq C$, meaning $w_{lin}$ satisfies the constraint in the constrained problem, then $w_{lin}$ is the solution to the constrained problem.\n",
    "\n",
    "It follows that $w_{lin} = w_{reg}$.\n",
    "\n",
    "Answer is [a].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[a] written as hard-order constraints.\n",
    "\n",
    "- Not true as the soft order constraint does not limit the orders.\n",
    "\n",
    "[b] translated into augmented error. \n",
    "\n",
    "- This is true as the constrained problem is equivalent to the augmented error unconstrained problem. The soft-order constraint in the constrained problem becomes a penalty term in the augmented error unconstrained problem.\n",
    "\n",
    "[c] determined from the value of the VC dimension.\n",
    "\n",
    "- Not true. \n",
    "\n",
    "[d] used to decrease both $E_{in}$ and $E_{out}$.\n",
    "\n",
    "- Not true. It is used to decrease both $E_{out}$ (avoid overfitting) and the augmented error $E_{aug}$. The $E_{in}$ may increase.\n",
    " \n",
    "[e] None of the above is true.\n",
    "\n",
    "- Not true.\n",
    "\n",
    "Answer is [b].\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp7a.png)\n",
    "![](final_images/finalp7b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()d\n",
    "import time\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems 7-10\n",
    "# linear regression for classification with regularization\n",
    "\n",
    "# load training and test sets into pandas dataframes\n",
    "def load_data():\n",
    "    column_names = [\"digit\", \"intensity\", \"symmetry\"]\n",
    "    sep = '\\s+'\n",
    "    data_train = pd.read_csv(\n",
    "            \"http://www.amlbook.com/data/zip/features.train\", \\\n",
    "            sep=sep, names=column_names)\n",
    "    data_test = pd.read_csv(\n",
    "            \"http://www.amlbook.com/data/zip/features.test\", \\\n",
    "            sep=sep, names=column_names)\n",
    "    return data_train, data_test\n",
    "\n",
    "data_train, data_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output labels for classifiers\n",
    "classifiers_one_vs_all = dict({\n",
    "        \"0_vs_all\": 0,\n",
    "        \"1_vs_all\": 1,\n",
    "        \"2_vs_all\": 2,\n",
    "        \"3_vs_all\": 3,\n",
    "        \"4_vs_all\": 4,\n",
    "        \"5_vs_all\": 5,\n",
    "        \"6_vs_all\": 6,\n",
    "        \"7_vs_all\": 7,\n",
    "        \"8_vs_all\": 8,\n",
    "        \"9_vs_all\": 9\n",
    "        })\n",
    "\n",
    "classifier_1_vs_5 = {\"1_vs_5\": [1,5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one-vs-all dataframe\n",
    "def make_one_vs_all_df(df, classifiers):\n",
    "    # add binary labels to dataframe\n",
    "    one_vs_all = pd.DataFrame(df, copy=True)\n",
    "    for class_label, digit in classifiers.items():\n",
    "        # get the series for a digit\n",
    "        labels = one_vs_all.loc[one_vs_all[\"digit\"] == digit, \"digit\"]\n",
    "        # change the digit number into 1.0\n",
    "        labels.loc[:] = 1.0\n",
    "        # add a classifier column to the dataframe\n",
    "        one_vs_all[class_label] = labels\n",
    "    # fill in the NaN locations (corresponding to rest of digits) with -1.0    \n",
    "    one_vs_all.fillna(-1.0, inplace=True)\n",
    "    return one_vs_all\n",
    "\n",
    "data_train_one_vs_all = make_one_vs_all_df(data_train, classifiers_one_vs_all)\n",
    "data_test_one_vs_all = make_one_vs_all_df(data_test, classifiers_one_vs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one-vs-one dataframe\n",
    "# note: this doesn't work for more than one item in the classifiers dictionary\n",
    "def make_one_vs_one_df(df, classifiers):\n",
    "    # add binary labels to dataframe  \n",
    "    for class_label in classifiers.keys():\n",
    "        # get the two digits\n",
    "        digits = classifiers[class_label]\n",
    "        # filter just the rows for the two digits\n",
    "        df_trimmed = df.loc[df[\"digit\"].isin(digits),:]\n",
    "        one_vs_one = pd.DataFrame(df_trimmed, copy=True)\n",
    "        for digit in digits:\n",
    "            # get the series for just the fist digit\n",
    "            labels = one_vs_one.loc[one_vs_one[\"digit\"] == digit, \"digit\"]\n",
    "            # change the digit number into 1.0\n",
    "            labels.loc[:] = 1.0\n",
    "            # add a classifier column to the dataframe\n",
    "            one_vs_one[class_label] = labels\n",
    "            # ignore the second digit\n",
    "            break\n",
    "    # fill in the NaN locations (corresponding to second digit) with -1.0    \n",
    "    one_vs_one.fillna(-1.0, inplace=True)\n",
    "    return one_vs_one\n",
    "\n",
    "data_train_1_vs_5 = make_one_vs_one_df(data_train, classifier_1_vs_5)\n",
    "data_test_1_vs_5 = make_one_vs_one_df(data_test, classifier_1_vs_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the data set for a specific classifier\n",
    "def make_data_set(df, class_label):\n",
    "    inputs = np.array(df.loc[:, [\"intensity\", \"symmetry\"]])\n",
    "    outputs = np.array(df.loc[:, class_label])\n",
    "    data = np.column_stack((inputs, outputs))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularized least-squares linear regression for classiﬁcation\n",
    "class LinearRegression:\n",
    "    \n",
    "    def __init__(self, transform=False):\n",
    "        self.transform = transform\n",
    "        return\n",
    "    \n",
    "    def fit(self, inputs, outputs, _lambda=1.0):    \n",
    "        # get inputs and outputs\n",
    "        Z = self.transform_inputs(inputs)\n",
    "        y = outputs\n",
    "        # linear regression solution with regularization\n",
    "        d = np.size(Z, axis=1)\n",
    "        Z_pinv = np.linalg.inv(Z.T@Z + _lambda*np.eye(d))@Z.T\n",
    "        weights = np.dot(Z_pinv, y)\n",
    "        return weights\n",
    "    \n",
    "    def binary_error(self, weights, inputs, outputs):    \n",
    "        Z = self.transform_inputs(inputs)\n",
    "        y = outputs\n",
    "        g = [np.sign(np.dot(weights.T, z)) for z in Z]\n",
    "        return np.count_nonzero(g != y) / len(y)        \n",
    "         \n",
    "    # private\n",
    "    def transform_inputs(self, inputs):\n",
    "        ones = np.ones(len(inputs)).reshape(len(inputs), 1)\n",
    "        x1 = inputs[:, 0].reshape(len(inputs), 1)\n",
    "        x2 = inputs[:, 1].reshape(len(inputs), 1)\n",
    "        if self.transform:\n",
    "            return np.concatenate((ones, x1, x2, x1*x2, x1**2, x2**2), axis=1)\n",
    "        else:\n",
    "            return np.concatenate((ones, x1, x2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute E_in and E_out\n",
    "def in_out_sample_errors(classifiers, data_train, data_test=pd.DataFrame(), \n",
    "                         _lambda=1.0, transform=False):\n",
    "    results = [] \n",
    "    for class_label in classifiers.keys():\n",
    "            # get training data\n",
    "            data_set_train = make_data_set(data_train, class_label)\n",
    "            inputs_train = data_set_train[:, 0:2]\n",
    "            outputs_train = data_set_train[:, 2]          \n",
    "            # fit the model\n",
    "            lr = LinearRegression(transform)\n",
    "            weights = lr.fit(inputs_train, outputs_train, _lambda)         \n",
    "            # compute in-sample error\n",
    "            E_in = lr.binary_error(weights, inputs_train, outputs_train)       \n",
    "            # get testing data\n",
    "            E_out = None\n",
    "            if not data_test.empty:\n",
    "                data_set_test = make_data_set(data_test, class_label)\n",
    "                inputs_test = data_set_test[:, 0:2]\n",
    "                outputs_test = data_set_test[:, 2]       \n",
    "                # compute out-of-sample error\n",
    "                E_out =  lr.binary_error(weights, inputs_test, outputs_test) \n",
    "            # add to results\n",
    "            result = {\"classifier\": class_label,\n",
    "                      \"transform\": transform,\n",
    "                      \"lambda\": _lambda,\n",
    "                      \"E_in\": E_in,\n",
    "                      \"E_out\": E_out}\n",
    "            results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = in_out_sample_errors(classifiers_one_vs_all, data_train_one_vs_all, data_test_one_vs_all, \n",
    "                               _lambda=1.0, transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>transform</th>\n",
       "      <th>lambda</th>\n",
       "      <th>E_in</th>\n",
       "      <th>E_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0_vs_all</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109313</td>\n",
       "      <td>0.115097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1_vs_all</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015224</td>\n",
       "      <td>0.022422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2_vs_all</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100261</td>\n",
       "      <td>0.098655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3_vs_all</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.090248</td>\n",
       "      <td>0.082711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4_vs_all</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.089425</td>\n",
       "      <td>0.099651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5_vs_all</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.076258</td>\n",
       "      <td>0.079721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6_vs_all</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.091071</td>\n",
       "      <td>0.084704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7_vs_all</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088465</td>\n",
       "      <td>0.073244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8_vs_all</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.074338</td>\n",
       "      <td>0.082711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9_vs_all</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088328</td>\n",
       "      <td>0.088191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classifier  transform  lambda      E_in     E_out\n",
       "0   0_vs_all      False     1.0  0.109313  0.115097\n",
       "1   1_vs_all      False     1.0  0.015224  0.022422\n",
       "2   2_vs_all      False     1.0  0.100261  0.098655\n",
       "3   3_vs_all      False     1.0  0.090248  0.082711\n",
       "4   4_vs_all      False     1.0  0.089425  0.099651\n",
       "5   5_vs_all      False     1.0  0.076258  0.079721\n",
       "6   6_vs_all      False     1.0  0.091071  0.084704\n",
       "7   7_vs_all      False     1.0  0.088465  0.073244\n",
       "8   8_vs_all      False     1.0  0.074338  0.082711\n",
       "9   9_vs_all      False     1.0  0.088328  0.088191"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the `5_vs_all` through `9_vs_all` classifiers, the lowest `E_in` is found for `8_vs_all`.\n",
    "\n",
    "Answer is [d].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = in_out_sample_errors(classifiers_one_vs_all, data_train_one_vs_all, data_test_one_vs_all,\n",
    "                                _lambda=1.0, transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>transform</th>\n",
       "      <th>lambda</th>\n",
       "      <th>E_in</th>\n",
       "      <th>E_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0_vs_all</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102318</td>\n",
       "      <td>0.106627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1_vs_all</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012344</td>\n",
       "      <td>0.021923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2_vs_all</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100261</td>\n",
       "      <td>0.098655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3_vs_all</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.090248</td>\n",
       "      <td>0.082711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4_vs_all</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.089425</td>\n",
       "      <td>0.099651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5_vs_all</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.076258</td>\n",
       "      <td>0.079223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6_vs_all</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.091071</td>\n",
       "      <td>0.084704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7_vs_all</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088465</td>\n",
       "      <td>0.073244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8_vs_all</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.074338</td>\n",
       "      <td>0.082711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9_vs_all</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088328</td>\n",
       "      <td>0.088191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classifier  transform  lambda      E_in     E_out\n",
       "0   0_vs_all       True     1.0  0.102318  0.106627\n",
       "1   1_vs_all       True     1.0  0.012344  0.021923\n",
       "2   2_vs_all       True     1.0  0.100261  0.098655\n",
       "3   3_vs_all       True     1.0  0.090248  0.082711\n",
       "4   4_vs_all       True     1.0  0.089425  0.099651\n",
       "5   5_vs_all       True     1.0  0.076258  0.079223\n",
       "6   6_vs_all       True     1.0  0.091071  0.084704\n",
       "7   7_vs_all       True     1.0  0.088465  0.073244\n",
       "8   8_vs_all       True     1.0  0.074338  0.082711\n",
       "9   9_vs_all       True     1.0  0.088328  0.088191"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the `0_vs_all` through `4_vs_all` classifiers, the lowest `E_out` is found for `1_vs_all`.\n",
    "\n",
    "Answer is [b].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp9a.png)\n",
    "![](final_images/finalp9b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "[a] Overfitting always occurs when we use the transform. \n",
    "\n",
    "- Not true.\n",
    "\n",
    "[b] The transform always improves the out-of-sample performance by at least 5% ($E_{out}$ with transform $ \\leq 0.95E_{out}$ without transform).\n",
    "\n",
    "- Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7.359307\n",
       "1    2.222222\n",
       "2    0.000000\n",
       "3    0.000000\n",
       "4    0.000000\n",
       "5    0.625000\n",
       "6    0.000000\n",
       "7    0.000000\n",
       "8    0.000000\n",
       "9    0.000000\n",
       "Name: E_out, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(pd.DataFrame(results)['E_out'] - pd.DataFrame(results2)['E_out'])/pd.DataFrame(results)['E_out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not true: there is improvement in `E_out` only for `0_vs_all`, `1_vs_all`, and `5_vs_all`.\n",
    "\n",
    "[c] The transform does not make any difference in the out-of-sample performance. \n",
    "\n",
    "- Not true: There is improvement in `E_out` for `0_vs_all`, `1_vs_all`, and `5_vs_all`. \n",
    "\n",
    "[d] The transform always worsens the out-of-sample performance by at least 5%. \n",
    "\n",
    "- Not true.\n",
    "\n",
    "[e] The transform improves the out-of-sample performance of ‘5 versus all,’ but by less than 5%. \n",
    "\n",
    "- True, as the `5_vs_all` `E_out` performance is improved by 0.625%.\n",
    "\n",
    "Answer is [e].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\lambda = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  classifier  transform  lambda      E_in     E_out\n",
      "0     1_vs_5       True     1.0  0.005125  0.025943\n"
     ]
    }
   ],
   "source": [
    "result3 = in_out_sample_errors(classifier_1_vs_5, data_train_1_vs_5, data_test_1_vs_5, _lambda=1.0, transform=True)\n",
    "print(pd.DataFrame(result3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\lambda = 0.01$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  classifier  transform  lambda      E_in     E_out\n",
      "0     1_vs_5       True    0.01  0.004484  0.028302\n"
     ]
    }
   ],
   "source": [
    "result4 = in_out_sample_errors(classifier_1_vs_5, data_train_1_vs_5, data_test_1_vs_5, _lambda=0.01, transform=True)\n",
    "print(pd.DataFrame(result4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[a] Overfitting occurs (from $\\lambda=1$ to $\\lambda=0.01$). \n",
    "\n",
    "- This is true as `E_out` increases.\n",
    "\n",
    "[b] The two classifiers have the same $E_{in}$.\n",
    "\n",
    "- Not true.\n",
    "\n",
    "[c] The two classifiers have the same $E_{out}$.\n",
    "\n",
    "- Not true.\n",
    "\n",
    "[d] When $\\lambda$ goes up, both $E_{in}$ and $E_{out}$ go up.\n",
    "\n",
    "- Not true. When `lambda` ⬆️, `E_in` ⬆️, and `E_out` ⬇️.\n",
    "\n",
    "[e] When $\\lambda$ goes up, both $E_{in}$ and $E_{out}$ go down.\n",
    "\n",
    "- Not true. When `lambda` ⬆️, `E_in` ⬆️, and `E_out` ⬇️.\n",
    "\n",
    "Answer is [a].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "    \n",
    "# data set for problems 11-12\n",
    "def get_training_set():\n",
    "    return np.array([1., 0., -1.,\n",
    "                     0., 1., -1.,\n",
    "                     0., -1., -1.,\n",
    "                     -1., 0., 1.,\n",
    "                     0., 2., 1.,\n",
    "                     0., -2., 1.,\n",
    "                     -2., 0., 1.]).reshape(7, 3)\n",
    "    \n",
    "train_data = get_training_set()\n",
    "train_inputs = train_data[:, 0:2]\n",
    "train_outputs = train_data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inputs(inputs):\n",
    "    x1 = inputs[:, 0].reshape(len(inputs), 1)\n",
    "    x2 = inputs[:, 1].reshape(len(inputs), 1)\n",
    "    return np.concatenate((x2**2 - 2*x1 - 1, x1**2 - 2*x2 + 1), axis=1)\n",
    "    \n",
    "train_inputs_transformed = transform_inputs(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 11\n",
    "# labels to the plots use tex rendering\n",
    "def plot_data(features, label, features_names=['$x_{1}$', '$x_{2}$'],\n",
    "              plot_hyperplanes=False):\n",
    "    plt.scatter(features[:, 0], features[:, 1], c=label, cmap='rainbow');\n",
    "    plt.xlabel(features_names[0]);\n",
    "    plt.ylabel(features_names[1]);\n",
    "    if plot_hyperplanes:\n",
    "        eps = 1e-12 # to prevent divide by zero\n",
    "        xfit = np.linspace(-3.5, 3.5)\n",
    "        for w1, w2, b in [(-1., 1., -0.5), (1., -1., -0.5), (1., 0., -0.5), \n",
    "                          (0., 1., -0.5)]:\n",
    "            slope = -w1 / (w2+eps)\n",
    "            intercept = -b / (w2+eps)\n",
    "            yfit =  slope * xfit + intercept\n",
    "            plt.plot(xfit, yfit, label=f\"w1 = {w1}, w2 = {w2}, b = {b}\")\n",
    "            plt.legend(loc='upper left')\n",
    "        plt.xlim(-3.5, 3.5);\n",
    "        plt.ylim(-3.5, 5.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperplane separates the data only if for $n = 1,\\cdots,7$:\n",
    "\n",
    "$$y_n(w^T z + b) > 0$$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data(train_inputs, train_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen the original training data in $X$ space is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data(train_inputs_transformed, train_outputs, features_names=['$z_{1}$', '$z_{2}$'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed training data in $Z$ space is now linearly separable.\n",
    "\n",
    "Plot the 4 hyperplanes for [a] through [d]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data(train_inputs_transformed, train_outputs, features_names=['$z_{1}$', '$z_{2}$'], plot_hyperplanes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot the only hyperplane that separates the data is [c].\n",
    "\n",
    "- It is orthogonal to the weight vector $w = [1.0, 0.0]$. \n",
    "\n",
    "- It separates the data with the maximum margin (as it bisects the closest points of the two classes), and that margin is 1.\n",
    "\n",
    "=> Answer is [c].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Implement hard margin SVM with the polynomial kernel using the `sklearn (libsvm)` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 12\n",
    "# hard margin SVM (set C = 1e10) with poly kernel, use sklearn package\n",
    "# no transform        \n",
    "def svm_hard_poly(Q):\n",
    "    clf = svm.SVC(C=1e10, kernel='poly', degree=Q, gamma=1.0, coef0=1.0)\n",
    "    return clf\n",
    "\n",
    "def problem_12():\n",
    "    clf = svm_hard_poly(2)\n",
    "    clf.fit(train_inputs, train_outputs)\n",
    "    num_support_vectors = np.size(clf.support_)\n",
    "    support_vectors = clf.support_vectors_\n",
    "    # dual_coef_ is the vector of products y_i*alpha_i\n",
    "    # alphas are positive, thus:\n",
    "    sv_outputs = np.sign(clf.dual_coef_)\n",
    "    print(f'b = {clf.intercept_}')\n",
    "    print(f\"alphas = \\n{sv_outputs*clf.dual_coef_}\")\n",
    "    print(f\"support vectors = \\n{support_vectors}\")\n",
    "    print(f\"# support vectors = {num_support_vectors}\")\n",
    "    return num_support_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = [-1.66633088]\n",
      "alphas = \n",
      "[[0.59647182 0.81065085 0.8887034  0.20566488 0.31275439]]\n",
      "support vectors = \n",
      "[[ 0.  1.]\n",
      " [ 0. -1.]\n",
      " [-1.  0.]\n",
      " [ 0.  2.]\n",
      " [ 0. -2.]]\n",
      "# support vectors = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_12()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is [c].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate solution to problem 12: implement hard margin SVM with polynomial or RBF kernel, using the `cvxopt` package.\n",
    "\n",
    "Also used in problems 13, 14, and 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems 13, 14, 15 (RBF kernel)\n",
    "# also used for an alternate solution to problem 12 (poly kernel)\n",
    "# uses cvxopt to solve the qp problem\n",
    "class SVM:\n",
    "\n",
    "    def __init__(self, kernel='poly', Q=2, gamma=1.0, tol_sv=1e-5):\n",
    "        if kernel not in ['poly','rbf']:\n",
    "            raise ValueError('Kernel must be \"poly\" or \"rbf\".')  \n",
    "        self.kernel = kernel\n",
    "        self.Q = Q\n",
    "        self.gamma = gamma\n",
    "        self.tol_sv = tol_sv # tightening increases # of support vectors\n",
    "        \n",
    "    def fit(self, inputs, outputs):\n",
    "        xn = inputs\n",
    "        yn = outputs\n",
    "        N = len(xn)\n",
    "\n",
    "        # construct quadratic coefficients matrix\n",
    "        mat = []\n",
    "        for row_idx in range(0, N):\n",
    "            for col_idx in range(0, N):\n",
    "                if self.kernel == 'poly':\n",
    "                    kernel = self.kernel_poly(xn[row_idx], xn[col_idx])\n",
    "                elif self.kernel == 'rbf':\n",
    "                    kernel = self.kernel_rbf(xn[row_idx], xn[col_idx])\n",
    "                val = yn[row_idx] * yn[col_idx] * kernel\n",
    "                mat.append(val)\n",
    "        mat = np.array(mat).reshape((N, N))\n",
    "        \n",
    "        # form matrices for quadratic programming solver\n",
    "        P = matrix(mat, tc='d')\n",
    "        q = matrix(-np.ones(N), tc='d')\n",
    "        b = matrix(0.0, tc='d')\n",
    "        A = matrix(yn, tc='d')\n",
    "        A = A.trans()\n",
    "        G = matrix(-np.identity(N), tc='d')\n",
    "        h = matrix(np.zeros(N), tc='d')\n",
    "                \n",
    "        # call qp solver to compute weights\n",
    "        solvers.options['show_progress'] = False # supress solver output  \n",
    "        sol = solvers.qp(P, q, G, h, A, b)\n",
    "        alpha = np.array(list(sol['x']))\n",
    "        \n",
    "        sv = []\n",
    "        sv_alphas = []\n",
    "        sv_outputs = []\n",
    "        for n in range(0, N):\n",
    "            if alpha[n] > self.tol_sv: # => xn[n] is support vector\n",
    "                sv.append(xn[n])\n",
    "                sv_alphas.append(alpha[n])\n",
    "                sv_outputs.append(yn[n])\n",
    "                    \n",
    "        # compute number of support vectors \n",
    "        num_sv = len(sv)\n",
    "        if (num_sv == 0):\n",
    "            raise Exception('There are no support vectors.')\n",
    "        \n",
    "        # compute b by averaging b's obtained from support vectors\n",
    "        bs = []\n",
    "        for m in range(0, num_sv):\n",
    "            b = sv_outputs[m]\n",
    "            for n in range(0, num_sv):\n",
    "                if self.kernel == 'poly':\n",
    "                    kernel = self.kernel_poly(sv[n], sv[m])\n",
    "                elif self.kernel == 'rbf':\n",
    "                    kernel = self.kernel_rbf(sv[n], sv[m])\n",
    "                b -= sv_alphas[n] * sv_outputs[n] * kernel\n",
    "            bs.append(b)\n",
    "        bs_avg = np.mean(bs)\n",
    "\n",
    "        return np.array(sv_alphas), np.array(sv), np.array(sv_outputs), bs_avg\n",
    "    \n",
    "    def binary_error(self, sv_alphas, sv, sv_outputs, b, inputs, outputs):\n",
    "        x = inputs\n",
    "        y = outputs\n",
    "        num_sv = len(sv)\n",
    "        \n",
    "        gs = []\n",
    "        for xm in x:\n",
    "            signal = 0.0\n",
    "            for n in range(0, num_sv):\n",
    "                if self.kernel == 'poly':\n",
    "                    kernel = self.kernel_poly(sv[n], xm)\n",
    "                elif self.kernel == 'rbf':\n",
    "                    kernel = self.kernel_rbf(sv[n], xm)\n",
    "                signal += sv_alphas[n] * sv_outputs[n] * kernel\n",
    "            signal += b\n",
    "            gs.append(signal)\n",
    "                \n",
    "        g = np.array(np.sign(gs))\n",
    "        return 100. * np.count_nonzero(y != g) / len(y)   \n",
    "    \n",
    "    # private\n",
    "    def kernel_poly(self, xn, xm):\n",
    "        return (1.0 + np.dot(xn.T, xm))**self.Q\n",
    "    \n",
    "    # private\n",
    "    def kernel_rbf(self, xn, xm):\n",
    "        return np.exp(-self.gamma * (np.linalg.norm(xn - xm)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 12 alternate solution    \n",
    "def problem_12_alt(tol_sv=1e-5):\n",
    "    svm = SVM(kernel='poly', tol_sv=tol_sv)\n",
    "    alphas, support_vectors, outputs, b = \\\n",
    "        svm.fit(train_inputs, train_outputs)\n",
    "    num_support_vectors = len(support_vectors)\n",
    "    print(f'b = {b}')\n",
    "    print(f\"alphas = \\n{alphas}\")\n",
    "    print(f\"support vectors = \\n{support_vectors}\")\n",
    "    print(f\"# support vectors = {num_support_vectors}\")\n",
    "    return num_support_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = -1.6666666856035512\n",
      "alphas = \n",
      "[0.7037037  0.7037037  0.88888889 0.25925926 0.25925926]\n",
      "support vectors = \n",
      "[[ 0.  1.]\n",
      " [ 0. -1.]\n",
      " [-1.  0.]\n",
      " [ 0.  2.]\n",
      " [ 0. -2.]]\n",
      "# support vectors = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_12_alt(tol_sv=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is [c].\n",
    "\n",
    "Note that tightening `tol_sv` from `1e-5` to `1e-12` increases the # of support vectors and gives the wrong answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = -2.0793650937934736\n",
      "alphas = \n",
      "[4.32455509e-09 7.03703704e-01 7.03703704e-01 8.88888891e-01\n",
      " 2.59259260e-01 2.59259260e-01 5.27081499e-10]\n",
      "support vectors = \n",
      "[[ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0. -1.]\n",
      " [-1.  0.]\n",
      " [ 0.  2.]\n",
      " [ 0. -2.]\n",
      " [-2.  0.]]\n",
      "# support vectors = 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_12_alt(tol_sv=1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp13a.png)\n",
    "![](final_images/finalp13b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems 13-18 common code\n",
    "def generate_training_inputs(N=100):\n",
    "    return np.array([np.random.uniform(-1.0, 1.0, 2) for n in range(0, N)])        \n",
    "\n",
    "def compute_targets(inputs):\n",
    "    x1 = inputs[:, 0]\n",
    "    x2 = inputs[:, 1]\n",
    "    return np.sign(x1 - x2 + 0.25 * np.sin(np.pi * x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 13\n",
    "# SVM hard margin with RBF kernel trials\n",
    "def run_svm_trials(N=100, n_trials=100, gamma=1.5, use_sklearn=False):\n",
    "    ein = 0.\n",
    "    non_sep_count = 0\n",
    "    \n",
    "    for n_trial in range(0, n_trials):\n",
    "        train_inputs = generate_training_inputs(N)\n",
    "        train_outputs = compute_targets(train_inputs)\n",
    "        \n",
    "        if use_sklearn == False: # use cvxopt\n",
    "            svm_cvxopt = SVM(kernel='rbf', gamma=gamma)\n",
    "            alphas, sv, sv_outputs, b = svm_cvxopt.fit(train_inputs, \\\n",
    "                                                       train_outputs)\n",
    "            ein = svm_cvxopt.binary_error(alphas, sv, sv_outputs, b, \\\n",
    "                                          train_inputs, train_outputs)\n",
    "            \n",
    "        elif use_sklearn == True: # use sklearn (libsvm)\n",
    "            clf = svm.SVC(C=1e10, kernel='rbf', gamma=1.0, coef0=1.0)\n",
    "            clf.fit(train_inputs, train_outputs)\n",
    "            model_outputs = clf.predict(train_inputs)\n",
    "            ein = np.count_nonzero(train_outputs != model_outputs) / N\n",
    "     \n",
    "        if ein > 0.0:\n",
    "            non_sep_count += 1\n",
    "    \n",
    "        perc_complete = 100*n_trial/n_trials\n",
    "        # this line does not work without the \\r in the beginning\n",
    "        print(f'\\rJob {perc_complete:3.1f}% complete.', end='\\r', flush=True)\n",
    "        \n",
    "    return non_sep_count / n_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_13(use_sklearn=False):\n",
    "    time_start = time.time()\n",
    "    n_trials=100\n",
    "    non_sep_percent = run_svm_trials(100, n_trials, use_sklearn=use_sklearn)\n",
    "    print(f\"The data set is not separable by the RBF kernel \\\n",
    "{round(non_sep_percent, 2)}% of the time in {n_trials} runs.\")\n",
    "    time_end = time.time()\n",
    "    print(f\"Run time = {(time_end - time_start):3.1f} seconds.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `cvxopt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set is not separable by the RBF kernel 0.0% of the time in 100 runs.\n",
      "Run time = 28.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_13(use_sklearn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sklearn (libsvm)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set is not separable by the RBF kernel 0.0% of the time in 100 runs.\n",
      "Run time = 0.5 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_13(use_sklearn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `sklearn` is **much faster** than `cvxopt`!\n",
    "\n",
    "Answer is [a].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Implement the Lloyd's algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 14\n",
    "# Lloyd's algorithm: cluster inputs into K clusters\n",
    "class KMeans:\n",
    "    \n",
    "    @staticmethod\n",
    "    def fit(inputs, K=3, n_runs = 1, max_iter=300, tol=0.0001, verbose=False):\n",
    "        # returns the tuple (Centroids of clusters (μ1, ..., μk) , \n",
    "        #                    cluster labels of inputs)\n",
    "        run_cluster_labels = []\n",
    "        run_centroids = []\n",
    "        \n",
    "        for run in range(0, n_runs):\n",
    "            \n",
    "            # initialize K centroids at random \n",
    "            # (note we could also initialize clusters instead)\n",
    "            centroids = generate_training_inputs(K)\n",
    "            prev_cluster_labels = []\n",
    "            \n",
    "            for iteration in range(0, max_iter):\n",
    "                if verbose:\n",
    "                    print('\\nIteration = ', iteration+1)\n",
    "                \n",
    "                # assign inputs to cluster with closest centroid\n",
    "                cluster_labels = []\n",
    "                for inputt in inputs:\n",
    "                    distances = []\n",
    "                    for centroid in centroids:\n",
    "                        distance = np.linalg.norm(inputt - centroid)\n",
    "                        distances.append(distance)\n",
    "                    cluster_num = np.argmin(distances)\n",
    "                    cluster_labels.append(cluster_num)\n",
    "                    if verbose:\n",
    "                        print(f\"{inputt} assigned to cluster {cluster_num}\")\n",
    "                if verbose:\n",
    "                    print('cluster labels = ', cluster_labels)\n",
    "\n",
    "                # if a cluster is empty discard the run\n",
    "                if set(cluster_labels) != set(range(0, K)):\n",
    "                    if verbose:\n",
    "                        print('\\nEmpty cluster. Discard this run.')\n",
    "                    return (np.array([]), np.array([]))\n",
    "\n",
    "                prev_centroids = centroids\n",
    "\n",
    "                # compute cluster centroids\n",
    "                centroids = []\n",
    "                for cluster_label in range(0, K):\n",
    "                    input_idxs = [i for i, label in enumerate(cluster_labels)\n",
    "                        if label == cluster_label]\n",
    "                    mean_cluster = np.mean(inputs[input_idxs], axis=0)\n",
    "                    if verbose:\n",
    "                        print(f'indices = {input_idxs}, \\\n",
    "mean cluster = {mean_cluster}')\n",
    "                    centroids.append(mean_cluster)              \n",
    "                centroids = np.array(centroids)\n",
    "                \n",
    "                # check for convergence\n",
    "                if verbose:\n",
    "                    print('prev_centroids = \\n', prev_centroids)\n",
    "                    print('centroids = \\n', centroids)\n",
    "                if iteration > 0: \n",
    "                    if np.linalg.norm(prev_centroids - centroids) < tol and \\\n",
    "                        prev_cluster_labels == cluster_labels:\n",
    "                        if verbose:\n",
    "                            print('\\nConverged')\n",
    "                        break\n",
    "\n",
    "                prev_cluster_labels = cluster_labels\n",
    "                \n",
    "            run_centroids.append(centroids)\n",
    "            run_cluster_labels.append(cluster_labels)\n",
    "            \n",
    "        high_count_index = KMeans.arg_max_run_clusters(run_cluster_labels, \\\n",
    "                                                       K, n_runs)\n",
    "        \n",
    "        return run_centroids[high_count_index], \\\n",
    "            np.array(run_centroids[high_count_index])\n",
    "    \n",
    "    # private\n",
    "    @staticmethod\n",
    "    def arg_max_run_clusters(run_cluster_labels, K, n_runs):\n",
    "        counts = {}\n",
    "        run_clusters = []\n",
    "        for run in range(0, len(run_cluster_labels)):\n",
    "            run_cluster = []\n",
    "            for k in range(0, K):\n",
    "                indices = [i for i, x in enumerate(run_cluster_labels[run]) \\\n",
    "                           if x == k]\n",
    "                run_cluster.append(indices)\n",
    "            run_cluster.sort()\n",
    "\n",
    "            if run_cluster not in run_clusters:\n",
    "                run_clusters.append(run_cluster)\n",
    "                counts[run] = 1\n",
    "            else:\n",
    "                run_clusters.append([])\n",
    "                index = run_clusters.index(run_cluster)\n",
    "                counts[index] += 1\n",
    "                \n",
    "        high_count_index = max(counts, key=counts.get)\n",
    "        return high_count_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Lloyd's algorithm on the small (N=7) data set from problems 11-12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test KMeans on the small (N=7) data set from problems 11-12\n",
    "def test_KMeans(K=3):\n",
    "    train_data = get_training_set()\n",
    "    train_inputs = train_data[:, 0:2]\n",
    "    mu, labels = KMeans.fit(train_inputs, K=K, verbose=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration =  1\n",
      "[1. 0.] assigned to cluster 1\n",
      "[0. 1.] assigned to cluster 1\n",
      "[ 0. -1.] assigned to cluster 1\n",
      "[-1.  0.] assigned to cluster 2\n",
      "[0. 2.] assigned to cluster 1\n",
      "[ 0. -2.] assigned to cluster 0\n",
      "[-2.  0.] assigned to cluster 2\n",
      "cluster labels =  [1, 1, 1, 2, 1, 0, 2]\n",
      "indices = [5], mean cluster = [ 0. -2.]\n",
      "indices = [0, 1, 2, 4], mean cluster = [0.25 0.5 ]\n",
      "indices = [3, 6], mean cluster = [-1.5  0. ]\n",
      "prev_centroids = \n",
      " [[-0.70167205 -0.35740014]\n",
      " [-0.21202989 -0.21797474]\n",
      " [-0.84683452 -0.32076596]]\n",
      "centroids = \n",
      " [[ 0.   -2.  ]\n",
      " [ 0.25  0.5 ]\n",
      " [-1.5   0.  ]]\n",
      "\n",
      "Iteration =  2\n",
      "[1. 0.] assigned to cluster 1\n",
      "[0. 1.] assigned to cluster 1\n",
      "[ 0. -1.] assigned to cluster 0\n",
      "[-1.  0.] assigned to cluster 2\n",
      "[0. 2.] assigned to cluster 1\n",
      "[ 0. -2.] assigned to cluster 0\n",
      "[-2.  0.] assigned to cluster 2\n",
      "cluster labels =  [1, 1, 0, 2, 1, 0, 2]\n",
      "indices = [2, 5], mean cluster = [ 0.  -1.5]\n",
      "indices = [0, 1, 4], mean cluster = [0.33333333 1.        ]\n",
      "indices = [3, 6], mean cluster = [-1.5  0. ]\n",
      "prev_centroids = \n",
      " [[ 0.   -2.  ]\n",
      " [ 0.25  0.5 ]\n",
      " [-1.5   0.  ]]\n",
      "centroids = \n",
      " [[ 0.         -1.5       ]\n",
      " [ 0.33333333  1.        ]\n",
      " [-1.5         0.        ]]\n",
      "\n",
      "Iteration =  3\n",
      "[1. 0.] assigned to cluster 1\n",
      "[0. 1.] assigned to cluster 1\n",
      "[ 0. -1.] assigned to cluster 0\n",
      "[-1.  0.] assigned to cluster 2\n",
      "[0. 2.] assigned to cluster 1\n",
      "[ 0. -2.] assigned to cluster 0\n",
      "[-2.  0.] assigned to cluster 2\n",
      "cluster labels =  [1, 1, 0, 2, 1, 0, 2]\n",
      "indices = [2, 5], mean cluster = [ 0.  -1.5]\n",
      "indices = [0, 1, 4], mean cluster = [0.33333333 1.        ]\n",
      "indices = [3, 6], mean cluster = [-1.5  0. ]\n",
      "prev_centroids = \n",
      " [[ 0.         -1.5       ]\n",
      " [ 0.33333333  1.        ]\n",
      " [-1.5         0.        ]]\n",
      "centroids = \n",
      " [[ 0.         -1.5       ]\n",
      " [ 0.33333333  1.        ]\n",
      " [-1.5         0.        ]]\n",
      "\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "test_KMeans(K=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement RBF model. It uses Lloyd's algorithm and computes the solution using the pseudo-inverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular RBF model classification, uses Lloyd's (KMeans) and pseudo-inverse\n",
    "class RBF:\n",
    "    \n",
    "    def __init__(self, gamma=1.0, K=3):\n",
    "        self.gamma = gamma\n",
    "        self.K = K\n",
    "    \n",
    "    def fit(self, inputs, outputs):\n",
    "       # returns the weights and the cluster centroids\n",
    "        x = inputs\n",
    "        y = outputs\n",
    "        N = len(x)\n",
    "        \n",
    "        # cluster the inputs to obtain centroids mu\n",
    "        mu, labels = KMeans.fit(inputs, self.K)\n",
    "        if np.size(mu) == 0:\n",
    "            return (np.array([]), np.array([]))\n",
    "        \n",
    "        # construct phi matrix\n",
    "        phi = []\n",
    "        for row_idx in range(0, N):\n",
    "            for col_idx in range(0, self.K):\n",
    "                kernel = self.kernel_rbf(x[row_idx], mu[col_idx])\n",
    "                phi.append(kernel)\n",
    "        phi = np.array(phi).reshape((N, self.K))\n",
    "        \n",
    "        # pseudo-inverse solution \n",
    "        weights = np.linalg.inv(phi.T@phi) @ (phi.T@y)\n",
    "        return weights, mu\n",
    "    \n",
    "    def binary_error(self, weights, centroids, inputs, outputs):\n",
    "        x = inputs\n",
    "        y = outputs\n",
    "        N = len(x)\n",
    "        mu = centroids\n",
    "        \n",
    "        # construct phi matrix\n",
    "        phi = []\n",
    "        for row_idx in range(0, N):\n",
    "            for col_idx in range(0, self.K):\n",
    "                kernel = self.kernel_rbf(x[row_idx], mu[col_idx])\n",
    "                phi.append(kernel)\n",
    "        phi = np.array(phi).reshape((N, self.K))\n",
    "        \n",
    "        # construct predictions and error percentage\n",
    "        signal = np.matmul(phi, weights)\n",
    "        g = np.sign(signal)\n",
    "        return 100. * np.count_nonzero(y != g) / len(y)   \n",
    "        \n",
    "    # private\n",
    "    def kernel_rbf(self, xn, muk):\n",
    "        return np.exp(-self.gamma * (np.linalg.norm(xn - muk)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regular_kernel_trials(N=100, N_test=10000, n_trials=100, \n",
    "                              K=9, gamma=1.5, use_sklearn=False):\n",
    "    non_sep_count = 0\n",
    "    empty_cluster_count = 0\n",
    "    wins_svm = 0\n",
    "    \n",
    "    for n_trial in range(0, n_trials):\n",
    "        # generate training data\n",
    "        train_inputs = generate_training_inputs(N)\n",
    "        train_outputs = compute_targets(train_inputs)\n",
    "        \n",
    "        # run SVM\n",
    "        if use_sklearn == False: # use cvxopt\n",
    "            svm_cvxopt = SVM(kernel='rbf', gamma=gamma)\n",
    "            alphas, sv, sv_outputs, b = \\\n",
    "                svm_cvxopt.fit(train_inputs, train_outputs)\n",
    "            ein = svm_cvxopt.binary_error(alphas, sv, sv_outputs, b, \\\n",
    "                                          train_inputs, train_outputs)\n",
    "            \n",
    "        elif use_sklearn == True: # use sklearn (libsvm)\n",
    "            clf = svm.SVC(C=1e10, kernel='rbf', gamma=1.0, coef0=1.0)\n",
    "            clf.fit(train_inputs, train_outputs)\n",
    "            model_outputs = clf.predict(train_inputs)\n",
    "            ein = np.count_nonzero(train_outputs != model_outputs) / N\n",
    "            \n",
    "        # check if non-separable\n",
    "        if ein > 0.0:\n",
    "            non_sep_count += 1\n",
    "            continue\n",
    "        \n",
    "        # run regular RBF\n",
    "        rbf = RBF(gamma, K)\n",
    "        weights, centroids = rbf.fit(train_inputs, train_outputs)\n",
    "        \n",
    "        # check if empty clusters\n",
    "        if np.size(weights) == 0:\n",
    "            empty_cluster_count += 1\n",
    "            continue\n",
    "        \n",
    "        # generate out of sample data\n",
    "        # out of sample size should be at least 100*N for reliable estimate\n",
    "        test_inputs = generate_training_inputs(N_test)\n",
    "        test_outputs = compute_targets(test_inputs)\n",
    "        \n",
    "        # compute and compare out of sample errors\n",
    "        if use_sklearn == False:\n",
    "            eout_svm = svm_cvxopt.binary_error(alphas, sv, sv_outputs, b, \\\n",
    "                                               test_inputs, test_outputs)\n",
    "        elif use_sklearn == True:\n",
    "            model_test_outputs = clf.predict(test_inputs)\n",
    "            eout_svm = np.count_nonzero(test_outputs != model_test_outputs) / N\n",
    "            \n",
    "        eout_rbf = rbf.binary_error(weights, centroids, test_inputs, \\\n",
    "                                    test_outputs) \n",
    "        \n",
    "        if eout_svm < eout_rbf:\n",
    "            wins_svm +=1\n",
    "            \n",
    "        perc_complete = 100*n_trial/n_trials\n",
    "        # this line does not work without the \\r in the beginning\n",
    "        print(f'\\rJob {perc_complete:3.1f}% complete.', end='\\r', flush=True)\n",
    "            \n",
    "    win_percent = \\\n",
    "        100. * wins_svm / (n_trials - non_sep_count - empty_cluster_count)\n",
    "    return (win_percent, non_sep_count, empty_cluster_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `problem_14()` uses `run_regular_kernel_trials()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_14(N_test, use_sklearn=False):\n",
    "    time_start = time.time()\n",
    "    n_trials = 100\n",
    "    K = 9\n",
    "    gamma = 1.5\n",
    "    N = 100\n",
    "    win_percent, non_sep_count, empty_cluster_count = \\\n",
    "        run_regular_kernel_trials(N, N_test, n_trials, K, gamma, use_sklearn)\n",
    "    runs = n_trials - non_sep_count - empty_cluster_count\n",
    "    print(f\"For K = {K} and γ = {gamma} the kernel form beats the regular \\\n",
    "form {round(win_percent, 2)}% of the time in {runs} runs ({non_sep_count} \\\n",
    "runs not separable, {empty_cluster_count} runs with empty clusters).\")\n",
    "    time_end = time.time()\n",
    "    print(f\"Run time = {(time_end - time_start):3.1f} seconds.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `cvxopt`, out of sample size `N_test = 1000`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K = 9 and γ = 1.5 the kernel form beats the regular form 80.9% of the time in 89 runs (0 runs not separable, 11 runs with empty clusters).\n",
      "Run time = 74.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_14(N_test=1000, use_sklearn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `cvxopt`, out of sample size `N_test = 10000`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K = 9 and γ = 1.5 the kernel form beats the regular form 80.65% of the time in 93 runs (0 runs not separable, 7 runs with empty clusters).\n",
      "Run time = 391.9 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_14(N_test=10000, use_sklearn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `sklearn (libsvm)`, out of sample size `N_test = 1000`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K = 9 and γ = 1.5 the kernel form beats the regular form 100.0% of the time in 87 runs (0 runs not separable, 13 runs with empty clusters).\n",
      "Run time = 33.3 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_14(N_test=1000, use_sklearn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `sklearn (libsvm)`, out of sample size `N_test = 10000`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K = 9 and γ = 1.5 the kernel form beats the regular form 90.91% of the time in 88 runs (0 runs not separable, 12 runs with empty clusters).\n",
      "Run time = 183.8 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_14(N_test=10000, use_sklearn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `sklearn` is **much faster** than `cvxopt`!\n",
    "\n",
    "Answer is [e].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the runs in **Google Colab** with `N_test = 10000` and `use_sklearn = True`.\n",
    "\n",
    "- Using runtime hardware accelerator = `None`:\n",
    "\n",
    "`For K = 9 and γ = 1.5 the kernel form beats the regular form 82.98% of the time in 94 runs (0 runs not separable, 6 runs with empty clusters).\n",
    "Run time = 82.4 seconds.`\n",
    "\n",
    "=> **2.6x faster**\n",
    "\n",
    "- Using runtime hardware accelerator = `GPU`:\n",
    "\n",
    "`For K = 9 and γ = 1.5 the kernel form beats the regular form 82.22% of the time in 90 runs (0 runs not separable, 10 runs with empty clusters).\n",
    "Run time = 67.5 seconds.`\n",
    "\n",
    "=> **3x faster**\n",
    "\n",
    "- Using runtime hardware accelerator = `TPU`:\n",
    "\n",
    "`For K = 9 and γ = 1.5 the kernel form beats the regular form 87.91% of the time in 91 runs (0 runs not separable, 9 runs with empty clusters).\n",
    "Run time = 78.7 seconds.`\n",
    "\n",
    "=> **2.5x faster**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `problem_15()` uses `run_regular_kernel_trials()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 15\n",
    "# same as problem_14(), just with different K\n",
    "def problem_15(N_test, use_sklearn=False):\n",
    "    time_start = time.time()\n",
    "    n_trials = 100\n",
    "    K = 12\n",
    "    gamma = 1.5\n",
    "    N = 100\n",
    "    win_percent, non_sep_count, empty_cluster_count = \\\n",
    "        run_regular_kernel_trials(N, N_test, n_trials, K, gamma, use_sklearn)\n",
    "    runs = n_trials - non_sep_count - empty_cluster_count\n",
    "    print(f\"For K = {K} and γ = {gamma} the kernel form beats the regular \\\n",
    "form {round(win_percent, 2)}% of the time in {runs} runs ({non_sep_count} \\\n",
    "runs not separable, {empty_cluster_count} runs with empty clusters).\")\n",
    "    time_end = time.time()\n",
    "    print(f\"Run time = {(time_end - time_start):3.1f} seconds.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `cvxopt`, out of sample size `N_test = 1000`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K = 12 and γ = 1.5 the kernel form beats the regular form 67.86% of the time in 84 runs (0 runs not separable, 16 runs with empty clusters).\n",
      "Run time = 113.5 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_15(N_test=1000, use_sklearn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `cvxopt`, out of sample size `N_test = 10000`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K = 12 and γ = 1.5 the kernel form beats the regular form 66.22% of the time in 74 runs (0 runs not separable, 26 runs with empty clusters).\n",
      "Run time = 408.3 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_15(N_test=10000, use_sklearn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `sklearn (libsvm)`, out of sample size `N_test = 1000`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K = 12 and γ = 1.5 the kernel form beats the regular form 100.0% of the time in 82 runs (0 runs not separable, 18 runs with empty clusters).\n",
      "Run time = 40.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_15(N_test=1000, use_sklearn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `sklearn (libsvm)`, out of sample size `N_test = 10000`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K = 12 and γ = 1.5 the kernel form beats the regular form 88.73% of the time in 71 runs (0 runs not separable, 29 runs with empty clusters).\n",
      "Run time = 198.1 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_15(N_test=10000, use_sklearn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is [d].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp16a.png)\n",
    "![](final_images/finalp16b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems 16 and 17\n",
    "def run_reg_rbf_param_trials(N=100, n_trials=100, Ks=[9], gammas=[1.5]):\n",
    "    empty_cluster_count = 0\n",
    "    ein_down_eout_up = 0\n",
    "    ein_up_eout_down = 0\n",
    "    both_up = 0\n",
    "    both_down = 0\n",
    "    both_same = 0\n",
    "    \n",
    "    for n_trial in range(0, n_trials):\n",
    "        eins = []\n",
    "        eouts = []\n",
    "        for K in Ks:\n",
    "            for gamma in gammas:\n",
    "                \n",
    "                # generate training data\n",
    "                train_inputs = generate_training_inputs(N)\n",
    "                train_outputs = compute_targets(train_inputs)\n",
    "\n",
    "                # run regular RBF\n",
    "                rbf = RBF(gamma, K)\n",
    "                weights, centroids = rbf.fit(train_inputs, train_outputs)\n",
    "\n",
    "                # check if empty clusters\n",
    "                if np.size(weights) == 0:\n",
    "                    empty_cluster_count += 1\n",
    "                    continue\n",
    "\n",
    "                # generate out of sample data\n",
    "                test_inputs = generate_training_inputs(N*10)\n",
    "                test_outputs = compute_targets(test_inputs)\n",
    "\n",
    "                # compute and compare values of in and out of sample errors\n",
    "                ein = rbf.binary_error(weights, centroids, \\\n",
    "                                       train_inputs, train_outputs) \n",
    "                eout = rbf.binary_error(weights, centroids, \\\n",
    "                                        test_inputs, test_outputs) \n",
    "                \n",
    "                eins.append(ein)\n",
    "                eouts.append(eout)\n",
    "        \n",
    "        # skip comparisons if empty cluster\n",
    "        if len(eins) < 2 or len(eouts) < 2:\n",
    "            continue\n",
    "        \n",
    "        # perform comparisons for Ein and Eout\n",
    "        # note: there are actually 9 possibilities\n",
    "        if (eins[1] > eins[0]) and (eouts[1] > eouts[0]):\n",
    "            both_up += 1\n",
    "        elif (eins[1] < eins[0]) and (eouts[1] < eouts[0]):\n",
    "            both_down += 1\n",
    "        elif (eins[1] > eins[0]) and (eouts[1] < eouts[0]):\n",
    "            ein_up_eout_down += 1\n",
    "        elif (eins[1] < eins[0]) and (eouts[1] > eouts[0]):\n",
    "            ein_down_eout_up += 1\n",
    "        else:\n",
    "            both_same += 1\n",
    "        \n",
    "        perc_complete = 100*n_trial/n_trials\n",
    "        # this line does not work without the \\r in the beginning\n",
    "        print(f'\\rJob {perc_complete:3.1f}% complete.', end='\\r', flush=True)\n",
    "        \n",
    "    counts = {'Ein goes down, but Eout goes up': ein_down_eout_up,\n",
    "              'Ein goes up, but Eout goes down ': ein_up_eout_down,\n",
    "              'Both Ein and Eout go up': both_up,\n",
    "              'Both Ein and Eout go down': both_down,\n",
    "              'Ein and Eout remain the same': both_same\n",
    "             }\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `problem_16()` uses `run_reg_rbf_param_trials()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the effect of K going from 9 to 12\n",
    "def problem_16():\n",
    "    time_start = time.time()\n",
    "    n_trials = 100\n",
    "    Ks=[9, 12]\n",
    "    gammas=[1.5]\n",
    "    counts = run_reg_rbf_param_trials(100, n_trials, Ks, gammas)\n",
    "    print(f'counts = {counts}')\n",
    "    max_counts = max(counts, key=counts.get)\n",
    "    print(f\"For regular RBF with γ = {gammas[0]}. If we go from K = {Ks[0]} \\\n",
    "clusters to K = {Ks[1]} clusters then '{max_counts}' happens the most \\\n",
    "({counts[max_counts]} times) (excluding runs with empty clusters).\")\n",
    "    time_end = time.time()\n",
    "    print(f\"Run time = {(time_end - time_start):3.1f} seconds.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts = {'Ein goes down, but Eout goes up': 6, 'Ein goes up, but Eout goes down ': 16, 'Both Ein and Eout go up': 8, 'Both Ein and Eout go down': 21, 'Ein and Eout remain the same': 19}\n",
      "For regular RBF with γ = 1.5. If we go from K = 9 clusters to K = 12 clusters then 'Both Ein and Eout go down' happens the most (21 times) (excluding runs with empty clusters).\n",
      "Run time = 73.2 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is [d].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Function `problem_17()` uses `run_reg_rbf_param_trials()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the effect of gamma going from 1.5 to 2\n",
    "def problem_17():\n",
    "    time_start = time.time()\n",
    "    n_trials = 500\n",
    "    Ks=[9]\n",
    "    gammas=[1.5, 2.0]\n",
    "    counts = run_reg_rbf_param_trials(100, n_trials, Ks, gammas)\n",
    "    print(f'counts = {counts}')\n",
    "    max_counts = max(counts, key=counts.get)\n",
    "    print(f\"For regular RBF with K = {Ks[0]} clusters. If we go from γ = \\\n",
    "{gammas[0]} to γ = {gammas[1]} then '{max_counts}' happens the most \\\n",
    "({counts[max_counts]} times) (excluding runs with empty clusters).\")\n",
    "    time_end = time.time()\n",
    "    print(f\"Run time = {(time_end - time_start):3.1f} seconds.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts = {'Ein goes down, but Eout goes up': 69, 'Ein goes up, but Eout goes down ': 63, 'Both Ein and Eout go up': 136, 'Both Ein and Eout go down': 71, 'Ein and Eout remain the same': 84}\n",
      "For regular RBF with K = 9 clusters. If we go from γ = 1.5 to γ = 2.0 then 'Both Ein and Eout go up' happens the most (136 times) (excluding runs with empty clusters).\n",
      "Run time = 454.4 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_17()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is [c].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 18\n",
    "def run_reg_rbf_trials(N=100, n_trials=100, K=9, gamma=1.5):\n",
    "    \n",
    "    empty_cluster_count = 0\n",
    "    ein_zero_count = 0\n",
    "    \n",
    "    for n_trial in range(0, n_trials):\n",
    "        # generate training data\n",
    "        train_inputs = generate_training_inputs(N)\n",
    "        train_outputs = compute_targets(train_inputs)\n",
    "        \n",
    "        # run regular RBF\n",
    "        rbf = RBF(gamma, K)\n",
    "        weights, centroids = rbf.fit(train_inputs, train_outputs)\n",
    "        \n",
    "        perc_complete = 100*n_trial/n_trials\n",
    "        # this line does not work without the \\r in the beginning\n",
    "        print(f'\\rJob {perc_complete:3.1f}% complete.', end='\\r', flush=True)\n",
    "        \n",
    "        # check if empty clusters\n",
    "        if np.size(weights) == 0:\n",
    "            empty_cluster_count += 1\n",
    "            continue\n",
    "        \n",
    "        # compute in sample error\n",
    "        ein = rbf.binary_error(weights, centroids, train_inputs, train_outputs) \n",
    "        \n",
    "        if ein == 0.0:\n",
    "            ein_zero_count +=1\n",
    "        \n",
    "    ein_zero_percent = 100. * ein_zero_count / (n_trials - empty_cluster_count)\n",
    "    return (ein_zero_percent, empty_cluster_count) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `problem_18()` uses `run_reg_rbf_trials()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_18(K=9):\n",
    "    time_start = time.time()\n",
    "    n_trials = 100\n",
    "    gamma = 1.5\n",
    "    ein_zero_percent, empty_cluster_count = \\\n",
    "        run_reg_rbf_trials(100, n_trials, K, gamma)\n",
    "    print(f\"The regular RBF achieves Ein = 0 with K = {K} and γ = {gamma}, \\\n",
    "{round(ein_zero_percent, 2)}% of the time (excluding {empty_cluster_count} \\\n",
    "runs with empty clusters).\")\n",
    "    time_end = time.time()\n",
    "    print(f\"Run time = {(time_end - time_start):3.1f} seconds.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regular RBF achieves Ein = 0 with K = 9 and γ = 1.5, 6.38% of the time (excluding 6 runs with empty clusters).\n",
      "Run time = 18.7 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is [a].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 18 note: \n",
    "\n",
    "Increasing `K` will:\n",
    "\n",
    "- increase the percentage of time getting `Ein=0`;\n",
    "- increase the number of runs with empty clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regular RBF achieves Ein = 0 with K = 12 and γ = 1.5, 11.49% of the time (excluding 13 runs with empty clusters).\n",
      "Run time = 22.1 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_18(K=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regular RBF achieves Ein = 0 with K = 20 and γ = 1.5, 48.65% of the time (excluding 63 runs with empty clusters).\n",
      "Run time = 16.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_18(K=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regular RBF achieves Ein = 0 with K = 30 and γ = 1.5, 0.0% of the time (excluding 97 runs with empty clusters).\n",
      "Run time = 7.5 seconds.\n"
     ]
    }
   ],
   "source": [
    "problem_18(K=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "- $P(h=f)$ is the prior, uniform over $h \\in [0,1]$.\n",
    "- $P(h=f|D)$ is the posterior.\n",
    "- $P(D|h=f)$ is the likelihood.\n",
    "\n",
    "Note that:\n",
    "\n",
    "$$P(h=f|D) = \\frac{P(D|h=f) P(h=f)}{P(D)} ∝ P(D|h=f) P(h=f)$$\n",
    "\n",
    "$P(D|h=f)$ is: given we have the proper hypothesis, what is the probability of our data? \n",
    "\n",
    "Our data set is just +1 for had heart attack and thus its probability is just simply the probability of predicting somebody getting a heart attack. \n",
    "\n",
    "Thus, as $h$ increases over $[0,1]$, the probability of predicting +1 increases over $[0,1]$ in a linear fashion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final_images/finalp20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Inequalities to possibly use:\n",
    " \n",
    "Inequality of arithmetic and geometric means (AM-GM inequality):\n",
    "\n",
    "$$\\sqrt{uv} \\leq \\frac{u+v}{2}$$ \n",
    "\n",
    "Cauchy inequality (vectors $u$ and $v$):\n",
    "\n",
    "$$|<u,v>| \\leq ||u|| ||v||$$\n",
    "\n",
    "Triangle inequality:\n",
    "\n",
    "$$||u+v|| \\leq ||u||+||v||$$\n",
    "\n",
    "Reverse triangle inequality:\n",
    "\n",
    "$$| ||u||-||v|| | \\leq ||u-v||$$\n",
    "\n",
    "Let's check case [c]:\n",
    "\n",
    "[c] $E_{out}(g)$ cannot be worse than the average of $E_{out}(g_1)$ and $E_{out}(g_2)$.\n",
    "\n",
    "$$N E_{out}(g_1) = (y-g_1)^2$$\n",
    "\n",
    "$$N E_{out}(g_2) = (y-g_2)^2$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "N E_{out}(g) & = \\left( y-\\frac{1}{2}(g_1+g_2) \\right)^2 \\\\\n",
    "& = y^2 - y(g_1+g_2) + \\frac{1}{4}(g_1+g_2)^2 \\\\\n",
    "& = y^2 - y g_1 - y g_2 + \\frac{1}{4}g_1^2 + \\frac{1}{4}g_2^2 + \\frac{1}{2}g_1 g_2 \\\\\n",
    "& = \\frac{1}{2}\\left[ 2 y^2 - 2 y g_1 - 2 y g_2 + \\frac{1}{2}g_1^2 + \\frac{1}{2}g_2^2 + g_1 g_2 \\right] \\\\ \n",
    "& = \\frac{1}{2}\\left[ (y^2 - 2 y g_1 + g_1^2) + (y^2 - 2 y g_2 + g_2^2) - \\frac{1}{2}g_1^2 - \\frac{1}{2}g_2^2 + g_1 g_2 \\right] \\\\\n",
    "& = \\frac{1}{2} \\left[ (y-g_1)^2 + (y-g_2)^2 - \\frac{1}{2}(g_1^2 + g_2^2 - 2 g_1 g_2) \\right] \\\\\n",
    "& = \\frac{N}{2}\\left[ E_{out}(g_1) + E_{out}(g_2) - \\frac{1}{2N} (g_1-g_2)^2 \\right]\n",
    "\\end{aligned}$$\n",
    "\n",
    "i.e. we get:\n",
    "\n",
    "$$E_{out}(g) = \\frac{1}{2}\\left[ E_{out}(g_1) + E_{out}(g_2) \\right] - \\frac{1}{4N} (g_1-g_2)^2$$\n",
    "\n",
    "So $E_{out}(g)$ is always less than the average of $E_{out}(g_1)$ and $E_{out}(g_2)$.\n",
    "\n",
    "The statement [c] is true.\n",
    "\n",
    "Answer is [c].\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
